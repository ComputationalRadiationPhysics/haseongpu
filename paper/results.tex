\section{Results}
\numberwithin{equation}{section}

\subsection{Setup of the testing environment}
\label{subsec:testingEnvironment}

The benchmarks were conducted on a GPU cluster consisting of 12 compute nodes,
each equipped with a quad core Intel Xeon CPU E5-2609 CPU (2.40GHz), 64GB RAM
and 4 NVIDIA Tesla K20M GPUs with 5GB GDDR5 RAM. Each GPU contains 2496 CUDA cores with a total
peak performance of 3520 GFLOP/s. Job submission is handled by TORQUE
(\cite{torque}) with Maui as scheduling backend. The modeled gain medium is a
\textbf{TODO: Daniel should check/complete the description of the medium} YAG
crystal with a square surface of $16cm^2$\textbf{(?)} and a thickness of
$0.6cm$\textbf{(?)}. Its upper surface was sampled with 321 points, resulting in
600 Delaunay triangles (see section \ref{subsec:meshSampling}). The mesh is
extruded 9 times on the vertical axis, amounting to a total of 3210 sample
points and 5400 prisms respectively.

\subsection{Benchmark of values}

\begin{itemize}

  \item TODO

  \item \textbf{Image: gain VS time for one single point. Overlay between
    experiment/daniels sim/our sim}
    \begin{figure}[H]
      \centerline{
        \resizebox{0.5\textwidth}{!}{\includegraphics{plot/benchmark.png}}}
      \caption{gain of single sample point in 50 timessteps}
      \label{plot:benchmark}
    \end{figure}


  \item comparison with previous values from Daniel's thesis

  \item (comparison with results from an experiment)

\end{itemize}


\subsection{Runtimes}
In Figure \ref{plot:runtime}, the runtimes of the original single threaded
algorithm from \cite{ASE2010} are compared to the developed non-adaptive
parallel ASE-flux algorithm with different numbers of rays to demonstrate
scaling for different workloads. All simulations were done without using
reflections. To increase the number of GPUs, MPI\cite{MPI} was used to distribute the
sample points to all available devices. 

%reflections. To increase the number of GPUs to 48, an \emph{array job} was
%scheduled and the sampling points distributed evenly to the 12 Nodes (each
%hosting 4 GPUs) as seen in section \ref{subsubsec:multigpu}. When using the CPU
%or up to 4 GPUs, the computation uses only a single node, which results in a
%linear scaling of the algorithm. If the cluster's job submission system is used,
%a significant runtime overhead can be seen as a result of the job scheduler
%taking several seconds to process a single job. Therefore, using a high number
%of GPUs can lead to suboptimal performance in otherwise fast computations.
%However, this becomes neglegible for very work intensive simulations.
\begin{figure}[H]
  \centerline{
    \resizebox{0.5\textwidth}{!}{\includegraphics{plot/runtime.png}}}
  \caption{runtime of orginial algorithm compared to parallel algorithm}
  \label{plot:runtime}
\end{figure}
For the adaptive algorithm, simulation times of different sample points can vary
significantly (see Section \ref{subsec:adaptive_sampling}). Therefore, a uniform
partitioning of the sample points among the nodes would result in an unbalanced
workload and reduced efficency for the whole computation. Instead, one of the
nodes acts as a \emph{head node} and manages workload distribution based on demand: As
soon as one of the \emph{compute nodes} is idle, it will request more data to simulate.
Apart from a constant initialization overhead, distributing the computation to
multiple devices scales well, even with the adaptive algorithm. (Figure
\ref{plot:gpu_scaling}).
\begin{figure}[H]
  \centerline{
    \resizebox{0.5\textwidth}{!}{\includegraphics{plot/scaling.png}}}
  \caption{efficiency on multiple devices}
  \label{plot:gpu_scaling}
\end{figure}
Adaptive sampling usually does not only eliminate outliers, but even reduce the
needed time to do so: By using the idea of a pre-defined $MSE$-threshold, the
precision of the simulation can be adjusted in terms of this threshold rather than
simply increasing the number of rays for all the sample points. Since only a
small subset of sample points actually needs to be sampled with a high
resolution, a small increase in runtime can be sufficient to lower the maximal
$MSE$ values below the desired threshold (Figure \ref{plot:adaptive_runtime}).
This can be adjusted to yield similar simulation results as the non-adaptive
implementation at a fraction of its runtime. Note that some values in the
graphic actually display almost the same runtime, since the computation always
succeeded to stay below the given threshold with very little additional effort.
\begin{figure}[H]
  \centerline{
    \resizebox{0.5\textwidth}{!}{\includegraphics{graphics/adaptive_runtime.png}}}
  \caption{runtime comparison of adaptive and non-adaptive }
  \label{plot:adaptive_runtime}
\end{figure}
\subsection{Limitations and future work}
\label{subsec:limitations}
Future work should address reflections on the lateral sides of the gain medium
to allow the simulation of lateral feedback. The current implementation only
supports reflections on the upper and lower surface. Furthermore, these upper
and lower surfaces need to be coplanar due to the structure of the mesh
(Section \ref{subsec:meshSampling}).
Another limitation originates from the fact that each simulation uses a mapping
from rays to their respective starting position. For a large number of rays,
this can lead to a substancial amout of allocated memory. With the current
hardware (Section \ref{subsec:testingEnvironment}) and implementation, each sample
point can be simulated with a maximum of $6.5\cdot10^8$ rays. To push this limit
further, a more sophisticated mapping needs to be implemented. This could
be done by simply splitting the mapping in different parts and calculating them
iteratively.
Lastly, the simulation of multiple wavelengths is not done in parallel but
rather sequentially. This would be algorithmically possible, but since the GPUs
are already saturated by a single wavelength, multiple parallel wavelengths on a
single GPU don't reduce overall simulation duration. In addition, the parallel
wavlength computation would need more GPU memory, which is already a bottleneck
as seen abovesaturated by a single wavelength, multiple parallel wavelengths on
a single GPU don't reduce overall simulation duration. In addition, the parallel
wavlength computation would need more GPU memory, which is already a bottleneck
as seen above. Therefore, multiple wavelengths would linearly reduce the number
of possible rays per sample point.
