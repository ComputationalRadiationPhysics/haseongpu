\section{Methods}
\numberwithin{equation}{section}
This section explains methods used to reduce runtime by parallelization and also
increase precision.

\subsection{Parallel methods}
\label{subsec:parallel_methods}
Moving from a single CPU to a multiple
GPU implementation is not so much about changing the overall algorithm,
but more about efficient utilisation of the GPU ressources. 
By using CUDA as a parallel computing platform created by NVIDIA,
it is possible to obtain high performance with a high level 
programming model (similar to C++). Therefore the parallelization was pushed on CUDA-thread, 
CUDA-block and multiple GPU level to reach a good speedup compared to 
the original implementation.
In the following, the used parallelization
techniques and their implementations are explained.

\subsubsection{Sample Points on multiple GPUs}
The sample points are independent from each other
and therefore its $\Phi_{ASE}(\vec{r}_0)$ can be calculated in parallel. Every $\Phi_{ASE}(\vec{r}_0)$
calculation is implemeneted as a own CUDA-Kernel, thus sample points can be partioned 
to several devices. This is done automatically to the available GPUs on
the same host by pthreads \cite{pthreads}. As a result the speedup for multiple
GPUs is almost linear, which was tested with up to 4 GPUs (see figure \ref{plot:gpu_scaling}).
    
\subsubsection{Rays as GPU threads}
    Every ray can be traced independently through the mesh structure.
    Exploiting this parallelism provides a great opportunity to boost
    performance. A CUDA-thread implements the raytracing with the 
    following cycle:
    \begin{enumerate}
      \item Request a prism to start ray from
      \item Generate start point inside the prism $r_i$
      \item Generate ray $\overrightarrow{r_ir_0}$
      \item Calculate $gain(\overrightarrow{r_ir_0})$ by \eqref{eq:gain}
      \item Add gain to $\sum_{gain}$ atomically
    \end{enumerate}
    This repeats until every ray was traced. Furthermore rays with same
    origin and direction will be grouped into warps to utilize the
    GPU cache in a good way.

\subsubsection{Wavelength as seperate block dimensions}
    According to \ref{subsec:monteCarlo}, the simulation of a polychromatic
    laser can be split into sufficiently small monochromatic simulations which
    are again completely independent from each other.


\subsection{Importance sampling}
\label{subsec:importance_sampling}
Importance sampling is a well known technique in the domain
of statistics \cite{importanceSamplingSource}. In case of 
ASE-Flux calculation, a presampling of a sample point is done
to figure out which areas are more important to the result
and thus need more rays then others. This method has a strong
impact on the precision of the simulation.

Figure \ref{graphic:importance} demonstrates
the differences between calculation without and with
importance sampling. It is shown each a cut through the active
gain medium in which the graphic without important
sampling has some peaks. Thus, importance 
sampling can increase the efficiency of Monte Carlo simulations 
by reducing variance and simulation runtime. 
\begin{figure}
  \centerline
  {\resizebox{0.45\textwidth}{!}{\includegraphics{graphics/importance.png}}}
  \caption{Cuts through activ gain medium: no importance sampling (top), with importance sampling (bottom)}
  \label{graphic:importance}
\end{figure}

\subsection{Adaptive sampling}
\label{subsec:adaptive_sampling}
Since most sample points behave in a good way, there is no need
to sample them with a high number of rays. Only some outliers need to
be sampled with a higher precision. To assess the precision
and therefore the difference between the true and calculated value,
it is further determined the mean squared error (MSE).
The adaptive method allows to remove strong peaks in the result
of the simulation without sampling all sample points with
a high number of rays resulting in reduced runtime.
\begin{figure}
  \centerline{
    \resizebox{0.5\textwidth}{!}{\includegraphics{plot/mse.png}}}
  \caption{Comparision no importance samling, not adaptive, adaptive}
  \label{plot:adaptive}
\end{figure}

In figure~\ref{plot:adaptive} the adaptive method with 
$5 \cdot 10^4$ to $5 \cdot 10^8$ rays per sample point is compared to the simulations
non adaptive and no importance sampling with each $3 \cdot 10^5$ rays per sample point (same runtime of 10s).
The not adaptive and no importance sampling method have some big peaks while all sample points of 
the adaptive method lie under a preset $MSE$-threshold (in this case 0.005). 

The adaptive method will not reduce the average MSE over sample points,
but it will reduce the maximal MSE values. Thus, using the adaptive
methods gives lower error by same calculation time.

%\begin{itemize}
%\item []
%     \[f(\vec{r_0}) = \frac{1}{n} \sum_{i=1}^n g_i \]
%     \[f^2(r_0) = \frac{1}{n} \sum_{i=1}^n g_i^2 \]
%     \[MSE(r_0) = \sqrt{\frac{f^2(r_0) - f(r_0)^2}{n}}\]
%\end{itemize}

